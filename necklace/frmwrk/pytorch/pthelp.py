"""Pytorch Help :p"""



"""
>>> b.

Display all 502 possibilities? (y or n)
b.T                           b.cholesky_solve(             b.floor_divide(               b.less_(                      b.nextafter(                  b.sin_(
b.abs(                        b.chunk(                      b.floor_divide_(              b.less_equal(                 b.nextafter_(                 b.sinh(
b.abs_(                       b.clamp(                      b.fmod(                       b.less_equal_(                b.nonzero(                    b.sinh_(
b.absolute(                   b.clamp_(                     b.fmod_(                      b.lgamma(                     b.norm(                       b.size(
b.absolute_(                  b.clamp_max(                  b.frac(                       b.lgamma_(                    b.normal_(                    b.slogdet(
b.acos(                       b.clamp_max_(                 b.frac_(                      b.log(                        b.not_equal(                  b.smm(
b.acos_(                      b.clamp_min(                  b.gather(                     b.log10(                      b.not_equal_(                 b.softmax(
b.acosh(                      b.clamp_min_(                 b.gcd(                        b.log10_(                     b.numel(                      b.solve(
b.acosh_(                     b.clip(                       b.gcd_(                       b.log1p(                      b.numpy(                      b.sort(
b.add(                        b.clip_(                      b.ge(                         b.log1p_(                     b.orgqr(                      b.sparse_dim(
b.add_(                       b.clone(                      b.ge_(                        b.log2(                       b.ormqr(                      b.sparse_mask(
b.addbmm(                     b.coalesce(                   b.geometric_(                 b.log2_(                      b.outer(                      b.sparse_resize_(
b.addbmm_(                    b.conj(                       b.geqrf(                      b.log_(                       b.output_nr                   b.sparse_resize_and_clear_(
b.addcdiv(                    b.contiguous(                 b.ger(                        b.log_normal_(                b.permute(                    b.split(
b.addcdiv_(                   b.copy_(                      b.get_device(                 b.log_softmax(                b.pin_memory(                 b.split_with_sizes(
b.addcmul(                    b.cos(                        b.grad                        b.logaddexp(                  b.pinverse(                   b.sqrt(
b.addcmul_(                   b.cos_(                       b.grad_fn                     b.logaddexp2(                 b.polygamma(                  b.sqrt_(
b.addmm(                      b.cosh(                       b.greater(                    b.logcumsumexp(               b.polygamma_(                 b.square(
b.addmm_(                     b.cosh_(                      b.greater_(                   b.logdet(                     b.pow(                        b.square_(
b.addmv(                      b.count_nonzero(              b.greater_equal(              b.logical_and(                b.pow_(                       b.squeeze(
b.addmv_(                     b.cpu(                        b.greater_equal_(             b.logical_and_(               b.prelu(                      b.squeeze_(
b.addr(                       b.cross(                      b.gt(                         b.logical_not(                b.prod(                       b.sspaddmm(
b.addr_(                      b.cuda(                       b.gt_(                        b.logical_not_(               b.put_(                       b.std(
b.align_as(                   b.cummax(                     b.half(                       b.logical_or(                 b.q_per_channel_axis(         b.stft(
b.align_to(                   b.cummin(                     b.hardshrink(                 b.logical_or_(                b.q_per_channel_scales(       b.storage(
b.all(                        b.cumprod(                    b.has_names(                  b.logical_xor(                b.q_per_channel_zero_points(  b.storage_offset(
b.allclose(                   b.cumsum(                     b.heaviside(                  b.logical_xor_(               b.q_scale(                    b.storage_type(
b.amax(                       b.data                        b.heaviside_(                 b.logit(                      b.q_zero_point(               b.stride(
b.amin(                       b.data_ptr(                   b.histc(                      b.logit_(                     b.qr(                         b.sub(
b.angle(                      b.deg2rad(                    b.hypot(                      b.logsumexp(                  b.qscheme(                    b.sub_(
b.any(                        b.deg2rad_(                   b.hypot_(                     b.long(                       b.quantile(                   b.subtract(
b.apply_(                     b.dense_dim(                  b.i0(                         b.lstsq(                      b.rad2deg(                    b.subtract_(
b.arccos(                     b.dequantize(                 b.i0_(                        b.lt(                         b.rad2deg_(                   b.sum(
b.arccos_(                    b.det(                        b.ifft(                       b.lt_(                        b.random_(                    b.sum_to_size(
b.arccosh(                    b.detach(                     b.imag                        b.lu(                         b.real                        b.svd(
b.arccosh_(                   b.detach_(                    b.index_add(                  b.lu_solve(                   b.reciprocal(                 b.symeig(
b.arcsin(                     b.device                      b.index_add_(                 b.map2_(                      b.reciprocal_(                b.t(
b.arcsin_(                    b.diag(                       b.index_copy(                 b.map_(                       b.record_stream(              b.t_(
b.arcsinh(                    b.diag_embed(                 b.index_copy_(                b.masked_fill(                b.refine_names(               b.take(
b.arcsinh_(                   b.diagflat(                   b.index_fill(                 b.masked_fill_(               b.register_hook(              b.tan(
b.arctan(                     b.diagonal(                   b.index_fill_(                b.masked_scatter(             b.reinforce(                  b.tan_(
b.arctan_(                    b.digamma(                    b.index_put(                  b.masked_scatter_(            b.relu(                       b.tanh(
b.arctanh(                    b.digamma_(                   b.index_put_(                 b.masked_select(              b.relu_(                      b.tanh_(
b.arctanh_(                   b.dim(                        b.index_select(               b.matmul(                     b.remainder(                  b.to(
b.argmax(                     b.dist(                       b.indices(                    b.matrix_exp(                 b.remainder_(                 b.to_dense(
b.argmin(                     b.div(                        b.int(                        b.matrix_power(               b.rename(                     b.to_mkldnn(
b.argsort(                    b.div_(                       b.int_repr(                   b.max(                        b.rename_(                    b.to_sparse(
b.as_strided(                 b.divide(                     b.inverse(                    b.maximum(                    b.renorm(                     b.tolist(
b.as_strided_(                b.divide_(                    b.irfft(                      b.mean(                       b.renorm_(                    b.topk(
b.as_subclass(                b.dot(                        b.is_coalesced(               b.median(                     b.repeat(                     b.trace(
b.asin(                       b.double(                     b.is_complex(                 b.min(                        b.repeat_interleave(          b.transpose(
b.asin_(                      b.dtype                       b.is_contiguous(              b.minimum(                    b.requires_grad               b.transpose_(
b.asinh(                      b.eig(                        b.is_cuda                     b.mm(                         b.requires_grad_(             b.triangular_solve(
b.asinh_(                     b.element_size(               b.is_distributed(             b.mode(                       b.reshape(                    b.tril(
b.atan(                       b.eq(                         b.is_floating_point(          b.movedim(                    b.reshape_as(                 b.tril_(
b.atan2(                      b.eq_(                        b.is_leaf                     b.mul(                        b.resize(                     b.triu(
b.atan2_(                     b.equal(                      b.is_meta                     b.mul_(                       b.resize_(                    b.triu_(
b.atan_(                      b.erf(                        b.is_mkldnn                   b.multinomial(                b.resize_as(                  b.true_divide(
b.atanh(                      b.erf_(                       b.is_nonzero(                 b.multiply(                   b.resize_as_(                 b.true_divide_(
b.atanh_(                     b.erfc(                       b.is_pinned(                  b.multiply_(                  b.retain_grad(                b.trunc(
b.backward(                   b.erfc_(                      b.is_quantized                b.mv(                         b.rfft(                       b.trunc_(
b.baddbmm(                    b.erfinv(                     b.is_same_size(               b.mvlgamma(                   b.roll(                       b.type(
b.baddbmm_(                   b.erfinv_(                    b.is_set_to(                  b.mvlgamma_(                  b.rot90(                      b.type_as(
b.bernoulli(                  b.exp(                        b.is_shared(                  b.name                        b.round(                      b.unbind(
b.bernoulli_(                 b.exp2(                       b.is_signed(                  b.names                       b.round_(                     b.unflatten(
b.bfloat16(                   b.exp2_(                      b.is_sparse                   b.nanquantile(                b.rsqrt(                      b.unfold(
b.bincount(                   b.exp_(                       b.isclose(                    b.nansum(                     b.rsqrt_(                     b.uniform_(
b.bitwise_and(                b.expand(                     b.isfinite(                   b.narrow(                     b.scatter(                    b.unique(
b.bitwise_and_(               b.expand_as(                  b.isinf(                      b.narrow_copy(                b.scatter_(                   b.unique_consecutive(
b.bitwise_not(                b.expm1(                      b.isnan(                      b.ndim                        b.scatter_add(                b.unsafe_chunk(
b.bitwise_not_(               b.expm1_(                     b.isneginf(                   b.ndimension(                 b.scatter_add_(               b.unsafe_split(
b.bitwise_or(                 b.exponential_(               b.isposinf(                   b.ne(                         b.select(                     b.unsafe_split_with_sizes(
b.bitwise_or_(                b.fft(                        b.isreal(                     b.ne_(                        b.set_(                       b.unsqueeze(
b.bitwise_xor(                b.fill_(                      b.istft(                      b.neg(                        b.sgn(                        b.unsqueeze_(
b.bitwise_xor_(               b.fill_diagonal_(             b.item(                       b.neg_(                       b.sgn_(                       b.values(
b.bmm(                        b.fix(                        b.kthvalue(                   b.negative(                   b.shape                       b.var(
b.bool(                       b.fix_(                       b.layout                      b.negative_(                  b.share_memory_(              b.vdot(
b.byte(                       b.flatten(                    b.lcm(                        b.nelement(                   b.short(                      b.view(
b.cauchy_(                    b.flip(                       b.lcm_(                       b.new(                        b.sigmoid(                    b.view_as(
b.ceil(                       b.fliplr(                     b.le(                         b.new_empty(                  b.sigmoid_(                   b.volatile
b.ceil_(                      b.flipud(                     b.le_(                        b.new_full(                   b.sign(                       b.where(
b.char(                       b.float(                      b.lerp(                       b.new_ones(                   b.sign_(                      b.zero_(
b.cholesky(                   b.floor(                      b.lerp_(                      b.new_tensor(                 b.signbit(                    
b.cholesky_inverse(           b.floor_(                     b.less(                       b.new_zeros(                  b.sin(                        
>>> 
"""




'''
>>> import torch
>>> from pttsutils import *
>>> a = torch.randn((3,4))
>>> a
tensor([[-1.2593, -0.9616,  2.1363, -1.3211],
        [ 0.6232, -0.6715, -0.1899,  0.1177],
        [-0.8954,  0.3600,  0.6011, -0.0511]])
>>> b = torch.randn((3,4), requires_grad=True)
>>> b
tensor([[ 1.4190,  0.4679,  0.7024, -0.2579],
        [ 1.2928, -0.5643,  0.2857, -0.7576],
        [-1.5270, -0.6147, -0.7652, -0.3463]])
>>> ab = pt_ts_vstack((a,b))
>>> ab
tensor([[-1.2593, -0.9616,  2.1363, -1.3211],
        [ 0.6232, -0.6715, -0.1899,  0.1177],
        [-0.8954,  0.3600,  0.6011, -0.0511],
        [ 1.4190,  0.4679,  0.7024, -0.2579],
        [ 1.2928, -0.5643,  0.2857, -0.7576],
        [-1.5270, -0.6147, -0.7652, -0.3463]])
>>> ab.grad_fn
<AsStridedBackward object at 0x7f681f1c0128>
>>> aaa = pt_ts_copy(ab)
>>> aaa
tensor([[-1.2593, -0.9616,  2.1363, -1.3211],
        [ 0.6232, -0.6715, -0.1899,  0.1177],
        [-0.8954,  0.3600,  0.6011, -0.0511],
        [ 1.4190,  0.4679,  0.7024, -0.2579],
        [ 1.2928, -0.5643,  0.2857, -0.7576],
        [-1.5270, -0.6147, -0.7652, -0.3463]])
>>> aaa.grad_fn
>>> aaa.requires_grad
True
>>> ab.requires_grad
True
>>> c = ab * 2
>>> c
tensor([[-2.5185, -1.9231,  4.2727, -2.6422],
        [ 1.2465, -1.3430, -0.3799,  0.2354],
        [-1.7909,  0.7200,  1.2023, -0.1022],
        [ 2.8380,  0.9359,  1.4048, -0.5159],
        [ 2.5856, -1.1286,  0.5713, -1.5152],
        [-3.0540, -1.2294, -1.5303, -0.6926]])
>>> c.grad_fn
<MulBackward0 object at 0x7f681f1c0128>
>>> cg = torch.ones(c.shape)
>>> cg
tensor([[ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]])
>>> c.backward(cg)
>>> c.grad
>>> ab.grad
>>> a.grad
>>> b.grad
tensor([[ 2.,  2.,  2.,  2.],
        [ 2.,  2.,  2.,  2.],
        [ 2.,  2.,  2.,  2.]])
>>> aaa.grad
>>> d = c + aaa * 3
>>> d
tensor([[ -6.2963,  -4.8078,  10.6817,  -6.6056],
        [  3.1162,  -3.3576,  -0.9497,   0.5885],
        [ -4.4772,   1.8001,   3.0057,  -0.2555],
        [  7.0950,   2.3397,   3.5121,  -1.2897],
        [  6.4641,  -2.8216,   1.4284,  -3.7880],
        [ -7.6350,  -3.0736,  -3.8258,  -1.7315]])
>>> aaa.grad
>>>
>>> dg = torch.ones(d.shape)
>>> d.backward(dg)
>>> aaa.grad
tensor([[ 3.,  3.,  3.,  3.],
        [ 3.,  3.,  3.,  3.],
        [ 3.,  3.,  3.,  3.],
        [ 3.,  3.,  3.,  3.],
        [ 3.,  3.,  3.,  3.],
        [ 3.,  3.,  3.,  3.]])
>>> c.grad
>>> a.grad
>>> b.grad
tensor([[ 4.,  4.,  4.,  4.],
        [ 4.,  4.,  4.,  4.],
        [ 4.,  4.,  4.,  4.]])
>>>
'''


# ============================================================================
# tensor grad / grad_fn
# ============================================================================
'''
(* is matmal)
a
b = w1 * a
c = w2 * a
m = w3 * b
n = w4 * c
d = m + n
l = 10. - d
'''
'''
Python 3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import torch
>>>
>>>
>>> a = torch.randn((3,4), requires_grad=True)
>>> a
tensor([[ 0.6693,  1.3373, -0.0514,  0.4411],
        [-0.6436,  0.1081,  0.5363, -1.0569],
        [-0.1184,  1.8925, -0.9172, -0.2041]])
>>> w1 = torch.randn((5,3), requires_grad=True)
>>> w1
tensor([[-0.0698, -0.3468,  0.9473],
        [-1.1140, -1.3642,  0.4436],
        [-0.6938,  0.8647, -0.7944],
        [-0.5938, -0.5925,  0.3202],
        [-0.4248,  0.2053, -0.0236]])
>>> w2 = torch.randn((5,3), requires_grad=True)
>>> w2
tensor([[-0.9375,  0.2347,  0.3835],
        [-0.5972, -0.9177,  1.6332],
        [ 0.7008, -2.2461,  0.5170],
        [ 0.2151, -1.0667,  0.5569],
        [-1.3056,  0.7758, -0.7569]])
>>>
>>> w3 = torch.randn((2,5), requires_grad=True)
>>> w3
tensor([[ 0.1744, -1.3246, -0.7490,  0.8450, -1.6910],
        [ 1.5353, -1.1158, -2.4053, -1.4014, -1.2417]])
>>> w4 = torch.randn((2,5), requires_grad=True)
>>> w4
tensor([[-0.4699,  4.3500,  0.1834, -0.1443, -0.8831],
        [ 1.3486,  1.2368, -0.7916, -0.1825,  0.3468]])
>>>
>>>
>>> b = torch.matmul(w1, a)
>>> b
tensor([[ 0.0644,  1.6620, -1.0513,  0.1424],
        [ 0.0799, -0.7978, -1.0813,  0.8599],
        [-0.9268, -2.3376,  1.2280, -1.0577],
        [-0.0540, -0.2522, -0.5809,  0.2989],
        [-0.4137, -0.5906,  0.1536, -0.3996]])
>>> c = torch.matmul(w2, a)
>>> m = torch.matmul(w3, b)
>>> n = torch.matmul(w4, c)
>>> m
tensor([[  1.2535,   3.8832,  -0.4215,   0.6063],
        [  2.8283,  10.1513,  -2.7378,   1.8805]])
>>> n
tensor([[  1.7397,  12.6379,  -9.6365,   3.3795],
        [ -3.1659,  -0.5865,  -0.6978,  -3.2093]])
>>> d = m + n
>>> d
tensor([[  2.9932,  16.5211, -10.0579,   3.9858],
        [ -0.3376,   9.5648,  -3.4356,  -1.3287]])
>>> l = 10. - d
>>> l
tensor([[  7.0068,  -6.5211,  20.0579,   6.0142],
        [ 10.3376,   0.4352,  13.4356,  11.3287]])
>>>
>>> l.grad
>>> d.grad
>>> m.grad
>>> n.grad
>>> c.grad
>>> b.grad
>>> a.grad
>>>
>>> l.grad_fn
<AddBackward0 object at 0x7ffa92784c50>
>>> d.grad_fn
<AddBackward1 object at 0x7ffa92784fd0>
>>> m.grad_fn
<MmBackward object at 0x7ffa92784c50>
>>> n.grad_fn
<MmBackward object at 0x7ffa92784fd0>
>>> c.grad_fn
<MmBackward object at 0x7ffa92784c50>
>>> b.grad_fn
<MmBackward object at 0x7ffa92784fd0>
>>> a.grad_fn


>>>
>>>
>>> gl = torch.ones(l.shape)
>>> l.backward(gl, create_graph=True)
>>> l.grad
>>> m.grad
>>> n.grad
>>> b.grad
>>> c.grad
>>> a.grad
tensor([[ -2.4073,  -2.4073,  -2.4073,  -2.4073],
        [  3.8861,   3.8861,   3.8861,   3.8861],
        [-12.3049, -12.3049, -12.3049, -12.3049]])
>>> w1.grad
tensor([[-4.0967,  1.8055, -1.1160],
        [ 5.8479, -2.5773,  1.5930],
        [ 7.5587, -3.3313,  2.0590],
        [ 1.3333, -0.5876,  0.3632],
        [ 7.0277, -3.0973,  1.9144]])
>>>


>>> bfn = b.grad_fn
>>> bfn
<MmBackward object at 0x7ffa92784fd0>
>>> dir(bfn)
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_register_hook_dict', 'next_functions', 'register_hook', 'requires_grad']
>>> bfn.next_functions
((<AccumulateGrad object at 0x7ffa927848d0>, 0), (<AccumulateGrad object at 0x7ffa92784ba8>, 0))
>>> bfn.next_functions[0]
(<AccumulateGrad object at 0x7ffa927848d0>, 0)
>>> bfn_f0 = bfn.next_functions[0]
>>> type(bfn_f0)
<class 'tuple'>
>>> type(bfn_f0[0])
<class 'AccumulateGrad'>
>>> lfn = l.grad_fn
>>> lfn
<AddBackward0 object at 0x7ffa92784f98>
>>> lfn.next_functions
((<NegBackward object at 0x7ffa9278f2b0>, 0),)
>>> lfn.next_functions[0][0]
<NegBackward object at 0x7ffa9278f2b0>
>>> dfn = d.grad_fn
>>> dfn
<AddBackward1 object at 0x7ffa9278f2e8>
>>> dfn.next_functions
((<MmBackward object at 0x7ffa9278f2b0>, 0), (<MmBackward object at 0x7ffa9278f080>, 0))
>>> cfn = c.grad_fn
>>> cfn
<MmBackward object at 0x7ffa9278f0f0>
>>> cfn.next_functions
((<AccumulateGrad object at 0x7ffa9278f2b0>, 0), (<AccumulateGrad object at 0x7ffa9278f080>, 0))
>>>
>>>
>>>
>>>
>>> a.data_ptr()
40216528
>>> str(a)
'tensor([[ 0.6693,  1.3373, -0.0514,  0.4411],\n        [-0.6436,  0.1081,  0.5363, -1.0569],\n        [-0.1184,  1.8925, -0.9172, -0.2041]])'
>>> id(a)
140714175934608
>>> hex(id(a))
'0x7ffa9278b090'
>>> hex(id(b))
'0x7ffa86768240'
>>> hex(id(c))
'0x7ffa86768288'
>>> hex(id(m))
'0x7ffa867680d8'
>>> hex(id(n))
'0x7ffa86768090'
>>> hex(id(d))
'0x7ffa86768168'
>>> hex(id(l))
'0x7ffa86768360'
>>>
>>>
>>>
>>> l.saved_tensors
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'Tensor' object has no attribute 'saved_tensors'
>>> lfn.saved_tensors
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'AddBackward0' object has no attribute 'saved_tensors'
>>> dfn.next_functions[1][0] == cfn.next_functions[1][0]
False
>>> dfn.next_functions[1][0] , cfn.next_functions[1][0]
(<MmBackward object at 0x7ffa9278f2b0>, <AccumulateGrad object at 0x7ffa9278f198>)
>>> cfn.next_functions
((<AccumulateGrad object at 0x7ffa9278f128>, 0), (<AccumulateGrad object at 0x7ffa9278f198>, 0))
>>> cfn
<MmBackward object at 0x7ffa9278f0f0>
>>> cfn.next_functions
((<AccumulateGrad object at 0x7ffa9278f128>, 0), (<AccumulateGrad object at 0x7ffa9278f198>, 0))
>>>
>>>
>>>


################## calculate grad (backprop) manually  ##################
# --------------------------------------
(* is matmal)
a
b = w1 * a
c = w2 * a
m = w3 * b
n = w4 * c
d = m + n
l = 10. - d
# --------------------------------------

>>> lg = torch.ones(d.shape)
>>> lg
tensor([[ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]])
>>> dg = lg * -1
>>> dg
tensor([[-1., -1., -1., -1.],
        [-1., -1., -1., -1.]])

>>> ###### NOTE: mg = ng = dg  [! ?]
>>> b
tensor([[ 0.0644,  1.6620, -1.0513,  0.1424],
        [ 0.0799, -0.7978, -1.0813,  0.8599],
        [-0.9268, -2.3376,  1.2280, -1.0577],
        [-0.0540, -0.2522, -0.5809,  0.2989],
        [-0.4137, -0.5906,  0.1536, -0.3996]])
>>> bt = b.t()
>>> bt
tensor([[ 0.0644,  0.0799, -0.9268, -0.0540, -0.4137],
        [ 1.6620, -0.7978, -2.3376, -0.2522, -0.5906],
        [-1.0513, -1.0813,  1.2280, -0.5809,  0.1536],
        [ 0.1424,  0.8599, -1.0577,  0.2989, -0.3996]])
>>> w3g = torch.matmul(dg, bt)
>>> w3g
tensor([[-0.8175,  0.9392,  3.0942,  0.5882,  1.2503],
        [-0.8175,  0.9392,  3.0942,  0.5882,  1.2503]])
>>> w3.grad
tensor([[-0.8175,  0.9392,  3.0942,  0.5882,  1.2503],
        [-0.8175,  0.9392,  3.0942,  0.5882,  1.2503]])
>>>
>>> ct = c.t()
>>> ct
tensor([[-0.8239, -0.0024,  1.8535,  0.7646, -1.2835],
        [-0.5026,  2.1929,  1.6726,  1.2263, -3.0944],
        [-0.1777, -1.9595, -1.7148, -1.0939,  1.1773],
        [-0.7398,  0.3731,  2.5775,  1.1086, -1.2413]])
>>> w4g = torch.matmul(dg, ct)
>>> w4g
tensor([[ 2.2440, -0.6041, -4.3888, -2.0056,  4.4419],
        [ 2.2440, -0.6041, -4.3888, -2.0056,  4.4419]])
>>> w4.grad
tensor([[ 2.2440, -0.6041, -4.3888, -2.0056,  4.4419],
        [ 2.2440, -0.6041, -4.3888, -2.0056,  4.4419]])
>>>
>>>
>>> w3t = w3.t()
>>> bg = torch.matmul(dg, w3t)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: size mismatch, m1: [2 x 4], m2: [5 x 2] at /pytorch/aten/src/TH/generic/THTensorMath.c:2033
>>> bg = torch.matmul(w3t, dg)
>>> bg
tensor([[-1.7096, -1.7096, -1.7096, -1.7096],
        [ 2.4404,  2.4404,  2.4404,  2.4404],
        [ 3.1543,  3.1543,  3.1543,  3.1543],
        [ 0.5564,  0.5564,  0.5564,  0.5564],
        [ 2.9327,  2.9327,  2.9327,  2.9327]])
>>>
>>> w4t = w4.t()
>>> cg = torch.matmul(w4t, dg)
>>>
>>> at = a.t()
>>>
>>> w1g = torch.matmul(bg, at)
>>> w1g
tensor([[-4.0967,  1.8055, -1.1160],
        [ 5.8479, -2.5773,  1.5930],
        [ 7.5587, -3.3313,  2.0590],
        [ 1.3333, -0.5876,  0.3632],
        [ 7.0277, -3.0973,  1.9144]])
>>> w1.grad
tensor([[-4.0967,  1.8055, -1.1160],
        [ 5.8479, -2.5773,  1.5930],
        [ 7.5587, -3.3313,  2.0590],
        [ 1.3333, -0.5876,  0.3632],
        [ 7.0277, -3.0973,  1.9144]])
>>>
>>> w2g = torch.matmul(cg, at)
>>> w2g
tensor([[ -2.1056,   0.9280,  -0.5736],
        [-13.3877,   5.9003,  -3.6469],
        [  1.4575,  -0.6423,   0.3970],
        [  0.7831,  -0.3451,   0.2133],
        [  1.2853,  -0.5664,   0.3501]])
>>> w2.grad
tensor([[ -2.1056,   0.9280,  -0.5736],
        [-13.3877,   5.9003,  -3.6469],
        [  1.4575,  -0.6423,   0.3970],
        [  0.7831,  -0.3451,   0.2133],
        [  1.2853,  -0.5664,   0.3501]])
>>>
>>>
>>> w1t = w1.t()
>>> w2t = w2.t()
>>>
>>> ag1 = torch.matmul(w1t, bg)
>>> ag1
tensor([[-6.3640, -6.3640, -6.3640, -6.3640],
        [ 0.2638,  0.2638,  0.2638,  0.2638],
        [-2.9339, -2.9339, -2.9339, -2.9339]])
>>> ag2 = torch.matmul(w2t, cg)
>>> ag2
tensor([[ 3.9567,  3.9567,  3.9567,  3.9567],
        [ 3.6224,  3.6224,  3.6224,  3.6224],
        [-9.3710, -9.3710, -9.3710, -9.3710]])
>>> ag = ag1 + ag2
>>> ag
tensor([[ -2.4073,  -2.4073,  -2.4073,  -2.4073],
        [  3.8861,   3.8861,   3.8861,   3.8861],
        [-12.3049, -12.3049, -12.3049, -12.3049]])
>>> a.grad
tensor([[ -2.4073,  -2.4073,  -2.4073,  -2.4073],
        [  3.8861,   3.8861,   3.8861,   3.8861],
        [-12.3049, -12.3049, -12.3049, -12.3049]])
>>>
>>>


##############  backprop graph (nodes and edges)  ##############
>>> afn = a.grad_fn
>>> afn
>>>
>>> bfn = b.grad_fn
>>> cfn = c.grad_fn
>>> mfn = m.grad_fn
>>> nfn = n.grad_fn
>>> dfn = d.grad_fn
>>> lfn = l.grad_fn
>>>
>>> lfn
<AddBackward0 object at 0x7ffa92784f98>
>>> dir(lfn)
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_register_hook_dict', 'next_functions', 'register_hook', 'requires_grad']
>>>
>>> lfn.next_functions
((<NegBackward object at 0x7ffa86764b70>, 0),)
>>>
>>> lfnf = lfn.next_functions[0][0]
>>> lfnf
<NegBackward object at 0x7ffa86764b70>
>>> dir(lfnf)
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_register_hook_dict', 'next_functions', 'register_hook', 'requires_grad']
>>>
>>> lfnf.next_functions
((<AddBackward1 object at 0x7ffa9278f2e8>, 0),)
>>>
>>> dfn
<AddBackward1 object at 0x7ffa9278f2e8>
>>>
>>> dfn == lfnf.next_functions[0][0]
True
>>>
>>> dfn.next_functions
((<MmBackward object at 0x7ffa86764c50>, 0), (<MmBackward object at 0x7ffa86764ac8>, 0))
>>>
>>> dfnf0 = dfn.next_functions[0][0]
>>> dfnf0
<MmBackward object at 0x7ffa86764c50>
>>> dfnf1 = dfn.next_functions[1][0]
>>> dfnf1
<MmBackward object at 0x7ffa86764ac8>
>>>
>>>
>>> mfn
<MmBackward object at 0x7ffa86764c50>
>>>
>>> mfn == dfnf0
True
>>>
>>> nfn
<MmBackward object at 0x7ffa86764ac8>
>>> nfn == dfnf1
True
>>>
>>> mfn.next_functions
((<AccumulateGrad object at 0x7ffa86764d68>, 0), (<MmBackward object at 0x7ffa92784fd0>, 0))
>>>
>>> mfnf0 = mfn.next_functions[0][0]
>>> mfnf1 = mfn.next_functions[1][0]
>>>
>>> mfnf1
<MmBackward object at 0x7ffa92784fd0>
>>> bfn
<MmBackward object at 0x7ffa92784fd0>
>>>
>>> bfn == mfnf1
True
>>>
>>> bfn == mfnf0
False
>>>
>>>
>>> w3.grad_fn
>>>
>>> mfnf0
<AccumulateGrad object at 0x7ffa86764d68>
>>> dir(mfnf0)
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_register_hook_dict', 'next_functions', 'register_hook', 'requires_grad', 'variable']
>>>
>>> mfnf0.next_functions
()
>>> mfnf0.variable
tensor([[ 0.1744, -1.3246, -0.7490,  0.8450, -1.6910],
        [ 1.5353, -1.1158, -2.4053, -1.4014, -1.2417]])
>>> w3
tensor([[ 0.1744, -1.3246, -0.7490,  0.8450, -1.6910],
        [ 1.5353, -1.1158, -2.4053, -1.4014, -1.2417]])
>>> mfnf0.variable is w3
True
>>>
>>>
'''


# ============================================================================
# tensor.retain_grad()
# ============================================================================
'''
>>> x = torch.randn((3,4), requires_grad=True)
>>> x
tensor([[-1.2350,  1.1919, -0.4096, -2.5236],
        [-0.4916, -1.3990,  1.0750,  0.3610],
        [ 1.1686, -1.5416,  1.7953, -1.8602]])
>>> w = torch.randn((5,3), requires_grad=True)
>>> w
tensor([[ 0.9497, -1.8167,  0.5534],
        [-0.1707, -1.4183,  0.6032],
        [-0.2938, -0.4580, -0.1614],
        [-1.2708, -0.6761,  1.6112],
        [-1.9641, -0.1389,  1.0965]])
>>> y = torch.matmul(w, x)
>>> y.retain_grad()
>>>
>>> y
tensor([[ 0.3670,  2.8206, -1.3486, -4.0820],
        [ 1.6129,  0.8508, -0.3718, -1.2034],
        [ 0.3995,  0.5393, -0.6617,  0.8763],
        [ 3.7846, -3.0527,  2.6863, -0.0344],
        [ 3.7752, -3.8371,  2.6237,  2.8667]])
>>>
>>> z = y + x * 2
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (5) must match the size of tensor b (3) at non-singleton dimension 0
>>>
>>> z = y * y
>>> z
tensor([[  0.1347,   7.9558,   1.8186,  16.6625],
        [  2.6016,   0.7239,   0.1382,   1.4481],
        [  0.1596,   0.2908,   0.4378,   0.7679],
        [ 14.3234,   9.3188,   7.2162,   0.0012],
        [ 14.2522,  14.7235,   6.8839,   8.2178]])
>>> z.grad_fn
<MulBackward1 object at 0x7ffa86764da0>
>>> y.grad_fn
<MmBackward object at 0x7ffa86764e48>
>>>
>>> x.grad
>>> w.grad
>>> y.grad
>>> z.grad
>>>
>>> zg = torch.ones(z.shape)
>>> zg
tensor([[ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.],
        [ 1.,  1.,  1.,  1.]])
>>>
>>> z.backward(zg, create_graph=True)
>>>
>>> z.grad
>>> y.grad
tensor([[ 0.7339,  5.6412, -2.6971, -8.1639],
        [ 3.2259,  1.7017, -0.7436, -2.4067],
        [ 0.7989,  1.0786, -1.3234,  1.7526],
        [ 7.5693, -6.1053,  5.3726, -0.0689],
        [ 7.5504, -7.6743,  5.2474,  5.7333]])
>>> x.grad
tensor([[-24.5368,  27.5816, -19.1795, -19.0310],
        [-12.4409,  -7.9620,   2.1992,  16.6925],
        [ 22.6978, -14.2778,  12.6828,  -0.0766]])
>>> w.grad
tensor([[ 27.5247, -14.0998,   2.5057],
        [  4.4226,  -5.6350,   4.2884],
        [ -3.5816,  -2.6916,  -6.3651],
        [-18.6519,  10.5707,  28.0308],
        [-35.0897,  14.7351,  19.4095]])
>>>
>>> yg = y * 2
>>> yg
tensor([[ 0.7339,  5.6412, -2.6971, -8.1639],
        [ 3.2259,  1.7017, -0.7436, -2.4067],
        [ 0.7989,  1.0786, -1.3234,  1.7526],
        [ 7.5693, -6.1053,  5.3726, -0.0689],
        [ 7.5504, -7.6743,  5.2474,  5.7333]])
>>> xt = x.t()
>>> wt = w.t()
>>>
>>> wg = torch.matmul(yg, xt)
>>> wg
tensor([[ 27.5247, -14.0998,   2.5057],
        [  4.4226,  -5.6350,   4.2884],
        [ -3.5816,  -2.6916,  -6.3651],
        [-18.6519,  10.5707,  28.0308],
        [-35.0897,  14.7351,  19.4095]])
>>>
>>> xg = torch.matmul(wt, yg)
>>> xg
tensor([[-24.5368,  27.5816, -19.1795, -19.0310],
        [-12.4409,  -7.9620,   2.1992,  16.6925],
        [ 22.6978, -14.2778,  12.6828,  -0.0766]])
>>>
'''
